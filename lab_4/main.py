import os
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from torch.optim import Adam

from dataloader import create_dataloader
from models import PoetryModel, PoetryType


class PoetryTrainer:
    def __init__(self, config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∏—Ö–æ–≤

        Args:
            config (dict): —Å–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        self.config = self._validate_config(config)
        self.device = self._setup_device()

        self.model = None
        self.optimizer = None
        self.criterion = None
        self.one_hot_embedding = None
        self.train_loader = None
        self.vocab = None
        self.current_epoch = 0
        self.best_loss = float('inf')

        # –ú–µ—Ç—Ä–∏–∫–∏
        self.train_losses = []
        self.validation_losses = []

        self._setup()

    def _validate_config(self, config):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        Args:
            config: –∏—Å—Ö–æ–¥–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

        Returns:
            dict: –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

        Raises:
            ValueError: –µ—Å–ª–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
        """
        required_params = ['batch_size', 'learning_rate', 'hidden_size', 'epochs']

        for param in required_params:
            if param not in config:
                raise ValueError(f"–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä '{param}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")

        default_config = {
            'dropout': 0.5,
            'model_path': 'model.pkl',
            'debug': False,
            'shuffle': True,
            'save_best_only': True,
            'early_stopping_patience': 10,
            'log_interval': 10,
            'validation_split': 0.1,
            'gradient_clip': 1.0
        }

        merged_config = {**default_config, **config}

        if merged_config['batch_size'] <= 0:
            raise ValueError(f"batch_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º, –ø–æ–ª—É—á–µ–Ω: {merged_config['batch_size']}")

        if merged_config['learning_rate'] <= 0:
            raise ValueError(f"learning_rate –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º, –ø–æ–ª—É—á–µ–Ω: {merged_config['learning_rate']}")

        if merged_config['epochs'] <= 0:
            raise ValueError(f"epochs –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º, –ø–æ–ª—É—á–µ–Ω: {merged_config['epochs']}")

        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è validated —É—Å–ø–µ—à–Ω–æ")
        return merged_config

    def _setup_device(self):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (GPU/CPU)

        Returns:
            torch.device: –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        """
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name()}")
        else:
            device = torch.device('cpu')
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

        return device

    def _setup(self):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")

        try:
            self._setup_data()

            self._setup_model()

            self._setup_training_components()

            self._load_existing_model()

            print("‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}")

    def _setup_data(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ
        """
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        self.train_loader, self.vocab = create_dataloader(
            batch_size=self.config['batch_size'],
            debug=self.config['debug'],
            shuffle=self.config['shuffle']
        )

        self.vocab_size = len(self.vocab.stoi)
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {self.vocab_size}")

        self.one_hot_embedding = nn.Embedding(
            self.vocab_size,
            self.vocab_size,
            _weight=torch.from_numpy(np.eye(self.vocab_size))
        ).to(self.device)

    def _setup_model(self):
        """
        –°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å
        """
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

        self.model = PoetryModel(
            vocab_size=self.vocab_size,
            hidden_size=self.config['hidden_size'],
            output_size=self.vocab_size,
            dropout=self.config['dropout']
        ).to(self.device)

        # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {total_params:,} (–æ–±—É—á–∞–µ–º—ã–µ: {trainable_params:,})")

    def _setup_training_components(self):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
        """
        print("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")

        self.optimizer = Adam(
            self.model.parameters(),
            lr=self.config['learning_rate']
        )

        self.criterion = nn.CrossEntropyLoss()

        print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam(lr={self.config['learning_rate']})")
        print(f"‚úÖ –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: CrossEntropyLoss")

    def _load_existing_model(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        """
        model_path = self.config['model_path']

        if os.path.exists(model_path):
            print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}...")

            try:
                self.model = torch.load(model_path, map_location=self.device)
                print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å.")
                self._setup_model()

    def train(self):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        model_path = self.config['model_path']

        if self._is_model_trained():
            print("‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.")
            return

        print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        print(f"üìà –≠–ø–æ—Ö: {self.config['epochs']}, Batch size: {self.config['batch_size']}")

        patience_counter = 0
        self.best_loss = float('inf')

        for epoch in range(self.config['epochs']):
            self.current_epoch = epoch

            train_loss = self.train_epoch(epoch)
            self.train_losses.append(train_loss)

            self._log_progress(epoch, train_loss)

            if self._should_save_model(train_loss):
                self._save_model(train_loss)
                patience_counter = 0
            else:
                patience_counter += 1

            if self._should_stop_early(patience_counter):
                print(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
                break

        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        self._print_training_summary()

    def _is_model_trained(self):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ–±—É—á–µ–Ω–∞ –ª–∏ —É–∂–µ –º–æ–¥–µ–ª—å

        Returns:
            bool: True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞
        """
        model_path = self.config['model_path']
        if os.path.exists(model_path) and not self.config['debug']:
            return True
        return False

    def train_epoch(self, epoch):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ

        Args:
            epoch: –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏

        Returns:
            float: —Å—Ä–µ–¥–Ω–∏–µ –ø–æ—Ç–µ—Ä–∏ –Ω–∞ —ç–ø–æ—Ö–µ
        """
        self.model.train()
        total_loss = 0
        total_batches = len(self.train_loader)

        progress_bar = tqdm(
            self.train_loader,
            desc=f'Epoch {epoch + 1}/{self.config["epochs"]}',
            leave=False
        )

        for batch_idx, batch in enumerate(progress_bar):
            try:
                self.optimizer.zero_grad()

                sentences = batch.sent.t().to(self.device)
                x, y = sentences[:, :-1], sentences[:, 1:]

                x_one_hot = self.one_hot_embedding(x).float()

                init_hidden = torch.zeros(1, len(x), self.config['hidden_size']).to(self.device)
                output, _ = self.model(x_one_hot, init_hidden)

                output_flat = output.reshape(-1, output.shape[-1])
                y_flat = y.flatten()
                loss = self.criterion(output_flat, y_flat)

                loss.backward()

                if self.config['gradient_clip'] > 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config['gradient_clip']
                    )

                self.optimizer.step()

                total_loss += loss.item()

                current_loss = total_loss / (batch_idx + 1)
                progress_bar.set_postfix({
                    'loss': f'{current_loss:.4f}',
                    'batch': f'{batch_idx + 1}/{total_batches}'
                })

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_idx}: {e}")
                continue

        avg_loss = total_loss / total_batches
        return avg_loss

    def _log_progress(self, epoch, train_loss):
        """
        –õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

        Args:
            epoch: –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
            train_loss: –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏
        """
        log_interval = self.config['log_interval']

        if (epoch + 1) % log_interval == 0 or epoch == 0:
            print(f"üìä –≠–ø–æ—Ö–∞ {epoch + 1}/{self.config['epochs']} - –ü–æ—Ç–µ—Ä–∏: {train_loss:.4f}")

    def _should_save_model(self, current_loss):
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª—å

        Args:
            current_loss: —Ç–µ–∫—É—â–∏–µ –ø–æ—Ç–µ—Ä–∏

        Returns:
            bool: True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
        """
        if not self.config['save_best_only']:
            return True

        return current_loss < self.best_loss

    def _save_model(self, current_loss):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å

        Args:
            current_loss: —Ç–µ–∫—É—â–∏–µ –ø–æ—Ç–µ—Ä–∏
        """
        model_path = self.config['model_path']

        try:
            torch.save(self.model, model_path)
            self.best_loss = current_loss
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–ø–æ—Ç–µ—Ä–∏: {current_loss:.4f})")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")

    def _should_stop_early(self, patience_counter):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏

        Args:
            patience_counter: —Å—á–µ—Ç—á–∏–∫ —Ç–µ—Ä–ø–µ–Ω–∏—è

        Returns:
            bool: True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
        """
        patience = self.config['early_stopping_patience']
        return patience > 0 and patience_counter >= patience

    def _print_training_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø–æ –æ–±—É—á–µ–Ω–∏—é"""
        if self.train_losses:
            initial_loss = self.train_losses[0]
            final_loss = self.train_losses[-1]
            improvement = initial_loss - final_loss

            print(f"\nüìà –°–≤–æ–¥–∫–∞ –æ–±—É—á–µ–Ω–∏—è:")
            print(f"   –ù–∞—á–∞–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏: {initial_loss:.4f}")
            print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏: {final_loss:.4f}")
            print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:.4f}")
            print(f"   –õ—É—á—à–∏–µ –ø–æ—Ç–µ—Ä–∏: {self.best_loss:.4f}")

    def generate_poetry(self, input_text, poetry_type=PoetryType.HIDDEN_HEAD, max_length=15):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

        Args:
            input_text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç (–¥–ª—èËóèÂ§¥ËØó- —Å—Ç—Ä–æ–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤)
            poetry_type: —Ç–∏–ø —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏—è
            max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

        Returns:
            str: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ

        Raises:
            ValueError: –µ—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
        """
        self.model.eval()

        if not input_text and poetry_type == PoetryType.HIDDEN_HEAD:
            raise ValueError("–î–ª—èËóèÂ§¥ËØó–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç")

        print(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∏—Ö–∞...")
        print(f"   –í—Ö–æ–¥: '{input_text}', –¢–∏–ø: {poetry_type.value}")

        try:
            input_tensor = self._prepare_input_tensor(input_text)

            with torch.no_grad():
                result = self.model.generate(
                    x=input_tensor,
                    vocab=self.vocab,
                    poetry_type=poetry_type,
                    sentence_count=len(input_text) if input_text else 4,
                    max_length=max_length
                )

            print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {result}")
            return result

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            raise

    def _prepare_input_tensor(self, input_text):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞

        Args:
            input_text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç

        Returns:
            torch.Tensor: –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä
        """
        if not input_text:
            random_idx = torch.randint(0, self.vocab_size, (1, 1))
            input_tensor = random_idx.to(self.device)
        else:

            try:
                char_indices = [self.vocab.stoi[char] for char in input_text]
            except KeyError as e:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª –≤ –≤—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ: {e}")

            input_tensor = torch.tensor(char_indices).unsqueeze(0).to(self.device)

        input_one_hot = self.one_hot_embedding(input_tensor).float()
        return input_one_hot

    def interactive_generation(self):
        """
        –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç–∏—Ö–æ–≤
        """
        print("\nüé≠ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∏—Ö–æ–≤")
        print("   –ö–æ–º–∞–Ω–¥—ã:")
        print("   - –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—èËóèÂ§¥ËØó")
        print("   - –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Å—Ç–∏—Ö–∞")
        print("   - –í–≤–µ–¥–∏—Ç–µ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞")

        while True:
            try:
                user_input = input("\nüìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç: ").strip()

                if user_input.lower() == 'quit':
                    print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break

                if user_input == '':
                    # –°–ª—É—á–∞–π–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
                    result = self.generate_poetry(
                        "",
                        poetry_type=PoetryType.BEGIN,
                        max_length=12
                    )
                else:
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—èËóèÂ§¥ËØó
                    result = self.generate_poetry(
                        user_input,
                        poetry_type=PoetryType.HIDDEN_HEAD,
                        max_length=15
                    )

            except KeyboardInterrupt:
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    """
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    config = {
        'batch_size': 32,
        'learning_rate': 0.001,
        'hidden_size': 128,
        'epochs': 200,
        'dropout': 0.5,
        'model_path': 'model.pkl',
        'debug': False,  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ True –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ç–ª–∞–¥–∫–∏
        'save_best_only': True,
        'early_stopping_patience': 10,
        'log_interval': 10,
        'gradient_clip': 1.0,
        'shuffle': True
    }

    print("=" * 50)
    print("üé≠ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–∏—Ç–∞–π—Å–∫–æ–π –ø–æ—ç–∑–∏–∏")
    print("=" * 50)

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = PoetryTrainer(config)

        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        trainer.train()

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        test_cases = [
            ("Ëä±ÂºÄÊúâÊÉÖ", PoetryType.HIDDEN_HEAD),
            ("ÊòéÊúàÊ∏ÖÈ£é", PoetryType.HIDDEN_HEAD),
            ("", PoetryType.BEGIN)  # –°–ª—É—á–∞–π–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        ]

        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
        print("-" * 30)

        for input_text, poetry_type in test_cases:
            try:
                result = trainer.generate_poetry(
                    input_text,
                    poetry_type,
                    max_length=15
                )
                print(f"‚úÖ –£—Å–ø–µ—Ö: '{input_text}' ‚Üí {result}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è '{input_text}': {e}")

        # –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        trainer.interactive_generation()

    except Exception as e:
        print(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
